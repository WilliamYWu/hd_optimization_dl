{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install  --quiet numpy matplotlib torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f38c3e08c10>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = torch.tensor(2.0)   # Curvature of the Utility Function\n",
    "rho = torch.tensor(0.04)    # Discount Rate\n",
    "A = torch.tensor(0.5)       # TFP\n",
    "alpha = torch.tensor(0.36)  # Returns to Scale\n",
    "delta = torch.tensor(0.05)  # Depreciation Rate of Capital\n",
    "\n",
    "n_hidden = 3\n",
    "n_neurons = 8\n",
    "batch_size = 1000\n",
    "number_epochs = 200000\n",
    "lowest_iteration = 0\n",
    "min_loss = 100000\n",
    "learning_rate = 0.001\n",
    "\n",
    "k_min = 0.1 # Lower bound of sample interval\n",
    "k_max = 10.0 # Upper bound of sample interval\n",
    "\n",
    "grid_size = 10000 # Plotting grid\n",
    "\n",
    "vf_init_guess = -60 # Value Function Initial Guess\n",
    "\n",
    "n_burned_iter = 0 # Ergodic distribution estimation\n",
    "\n",
    "np.random.seed(40)\n",
    "torch.manual_seed(40)\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feed_Forward_NN(nn.Module):\n",
    "    def __init__(self, n_hidden, n_neurons, init_guess):\n",
    "        super(Feed_Forward_NN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        # input layer\n",
    "        layers.extend([(nn.Linear(1, n_neurons)), nn.Tanh()])\n",
    "\n",
    "        # hidden layers\n",
    "        for _ in range(n_hidden - 1):\n",
    "            layers.extend([(nn.Linear(n_neurons, n_neurons)), nn.Tanh()])\n",
    "\n",
    "        # output layer\n",
    "        layers.extend([(nn.Linear(n_neurons, 1))])\n",
    "        layers[-1].bias.data.fill_(init_guess)\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up neural network\n",
    "VF = Feed_Forward_NN(n_hidden=n_hidden, n_neurons=n_neurons, init_guess=vf_init_guess)\n",
    "CF = Feed_Forward_NN(n_hidden=n_hidden, n_neurons=n_neurons, init_guess=0)\n",
    "\n",
    "# Define trainable network parameters\n",
    "theta_vf = VF.parameters()\n",
    "theta_cf = CF.parameters()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(list(theta_vf) + list(theta_cf), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HJB(k_capital, V, C):\n",
    "    output = V(k_capital)\n",
    "    gradients = torch.ones_like(output)\n",
    "    torch.autograd.backward(output, gradients)\n",
    "    v_prime = k_capital.grad\n",
    "\n",
    "    Y = A * torch.pow(k_capital, alpha)                             # Output\n",
    "    I = Y - torch.exp(C(k_capital))                                 # Investment\n",
    "    mu_k = I - delta * k_capital                                    # Capital drift\n",
    "    U = torch.pow(torch.exp(C(k_capital)), 1-gamma) / (1-gamma)     # Utility\n",
    "\n",
    "    HJB = U - rho * V(k_capital) + torch.multiply(torch.detach(v_prime), mu_k)\n",
    "    return HJB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ergodic_distribution(k_capital, V, C):\n",
    "    for i in range(n_burned_iter):\n",
    "        output = V(k_capital)\n",
    "        gradients = torch.ones_like(output)\n",
    "        torch.autograd.backward(output, gradients)\n",
    "        v_prime = k_capital.grad\n",
    "\n",
    "        v_prime_max = torch.max(v_prime, torch.tensor(1e-7))\n",
    "        Y = A * torch.pow(k_capital, alpha)\n",
    "        I = Y - torch.exp(C(k_capital))\n",
    "        dk_dt = I - delta * k_capital\n",
    "        dt = 0.1\n",
    "        k_capital = k_capital + dt * dk_dt\n",
    "    return k_capital \n",
    "\n",
    "# def boundary_condition_ergodic_1_step (k_capital, V, C):\n",
    "#     v_prime = torch.autograd.grad(V(k_capital), k_capital)[0]\n",
    "#     v_prime_max = torch.max(v_prime, torch.tensor(1e-7))\t\t# dV/dk\n",
    "#     Y = A * torch.pow(k_capital, alpha)\t\t\t                # Output\n",
    "#     C = torch.exp(C(k_capital))        \t\t\t                # Consumption\n",
    "#     I = Y - C \t\t\t\t\t\t\t\t\t                # Investment\n",
    "#     dK_dt = I - delta * k_capital  \t\t\t\t                # Capital drift\n",
    "#     dt = 0.1\n",
    "#     k_capital_t_plus_one = k_capital + dt * dK_dt\n",
    "\n",
    "#     # Require k_min < k (t + 1) < kMax \n",
    "#     error_lower_bound =  torch.max(torch.tensor([k_min]) - k_capital_t_plus_one, 0)\n",
    "#     error_upper_bound = torch.max(k_capital_t_plus_one - torch.tensor([k_max]), 0)\n",
    "#     error = error_lower_bound + error_upper_bound\n",
    "#     return error\n",
    "\n",
    "def boundary_condition(k_capital, V, C):\n",
    "    Y = A * torch.pow(k_capital, alpha)\t\t\t# Output\n",
    "    I = Y - torch.exp(C(k_capital)) \t\t\t# Investment\n",
    "    dK_dt = I - delta * k_capital  \t\t\t\t# Capital drift\n",
    "\n",
    "    epsilon = 1                                 # Values close enough to 0 can't have decreasing capital\n",
    "\n",
    "    error = torch.where((k_capital < epsilon) & (dK_dt < 0) , dK_dt, 0)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def C_error(k_capital, V, C):\n",
    "\toutput = V(k_capital)\n",
    "\tgradients = torch.ones_like(output)\n",
    "\ttorch.autograd.backward(output, gradients)\n",
    "\tv_prime = k_capital.grad\n",
    "\tv_prime_max = torch.max(v_prime, torch.tensor(1e-7))\n",
    "\tc_err = torch.pow(v_prime_max, (-1/gamma)) - torch.exp(C(k_capital))\n",
    "\treturn c_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(batchSize, k_min, k_max):\n",
    "    k_capital = torch.rand(batchSize, 1) * (k_max - k_min) + k_min\n",
    "    ergodic_k_capital = ergodic_distribution(k_capital, VF, CF)\n",
    "    ergodic_k_capital.requires_grad = True\n",
    "    errorV = HJB(ergodic_k_capital, VF, CF)\n",
    "    errorC = C_error(ergodic_k_capital, VF, CF)\n",
    "    errorB = boundary_condition(ergodic_k_capital, VF, CF)\n",
    "\n",
    "    lossV = torch.mean(torch.square(errorV))\n",
    "    lossC = torch.mean(torch.square(errorC))\n",
    "    lossB = torch.mean(torch.square(errorB))\n",
    "    total_loss = lossV + lossC + lossB\n",
    "    return lossV, lossC, lossB, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step():\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    lossV, lossC, lossB, total_loss = Loss(batch_size, k_min, k_max)\n",
    "    total_loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    return lossV, lossC, lossB, total_loss\n",
    "\n",
    "def train_model(epochs, min_loss):\n",
    "    losses_v = []\n",
    "    losses_c = []\n",
    "    losses_b = []\n",
    "    total_losses = []\n",
    "\n",
    "    best_vf = deepcopy(VF)\n",
    "    best_cf = deepcopy(CF)\n",
    "    lowest_iteration = 0\n",
    "    min_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        lossV, lossC, lossB, total_loss = training_step()\n",
    "\n",
    "        if total_loss < min_loss:\n",
    "            lowest_iteration = epoch\n",
    "            min_loss = total_loss\n",
    "            best_vf.load_state_dict(VF.state_dict())\n",
    "            best_vf.load_state_dict(CF.state_dict())\n",
    "            print(f\"\\nbest loss: {min_loss:.4f}\", end=\"\\r\")\n",
    "\n",
    "        losses_v.append(lossV.item())\n",
    "        losses_c.append(lossC.item())\n",
    "        losses_b.append(lossB.item())\n",
    "        total_losses.append(total_loss.item())\n",
    "\n",
    "    return losses_v, losses_c, losses_b, total_losses, lowest_iteration, min_loss, best_vf, best_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/if/research-gms/William/jesus/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py:200: UserWarning: Error detected in MulBackward0. Traceback of forward call that caused the error:\n",
      "  File \"/apps/Anaconda/2022.10/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/apps/Anaconda/2022.10/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/if/research-gms/William/jesus/.venv/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/if/research-gms/William/jesus/.venv/lib/python3.9/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/if/research-gms/William/jesus/.venv/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 728, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/if/research-gms/William/jesus/.venv/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/apps/Anaconda/2022.10/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/apps/Anaconda/2022.10/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/apps/Anaconda/2022.10/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/if/research-gms/William/jesus/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/if/research-gms/William/jesus/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/if/research-gms/William/jesus/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/if/research-gms/William/jesus/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/if/research-gms/William/jesus/.venv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/if/research-gms/William/jesus/.venv/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/if/research-gms/William/jesus/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/if/research-gms/William/jesus/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/if/research-gms/William/jesus/.venv/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/if/research-gms/William/jesus/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/if/research-gms/William/jesus/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/if/research-gms/William/jesus/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_157134/155099.py\", line 2, in <module>\n",
      "    lossesV, lossesC, lossesB, total_losses, min_loss_iteration, minimum_loss, best_VF, best_CF = train_model(number_epochs, min_loss)\n",
      "  File \"/tmp/ipykernel_157134/1845480623.py\", line 23, in train_model\n",
      "    lossV, lossC, lossB, total_loss = training_step()\n",
      "  File \"/tmp/ipykernel_157134/1845480623.py\", line 4, in training_step\n",
      "    lossV, lossC, lossB, total_loss = Loss(batch_size, k_min, k_max)\n",
      "  File \"/tmp/ipykernel_157134/219642449.py\", line 5, in Loss\n",
      "    errorV = HJB(ergodic_k_capital, VF, CF)\n",
      "  File \"/tmp/ipykernel_157134/1595477317.py\", line 12, in HJB\n",
      "    HJB = U - rho * V(k_capital) + torch.multiply(torch.detach(v_prime), mu_k)\n",
      " (Triggered internally at ../torch/csrc/autograd/python_anomaly_mode.cpp:114.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1000, 1]] is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[175], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Run Model (and output loss evolution)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m lossesV, lossesC, lossesB, total_losses, min_loss_iteration, minimum_loss, best_VF, best_CF \u001b[39m=\u001b[39m train_model(number_epochs, min_loss)\n\u001b[1;32m      4\u001b[0m VF\u001b[39m.\u001b[39mload_state_dict(best_VF\u001b[39m.\u001b[39mstate_dict())\n\u001b[1;32m      5\u001b[0m CF\u001b[39m.\u001b[39mload_state_dict(best_CF\u001b[39m.\u001b[39mstate_dict())\n",
      "Cell \u001b[0;32mIn[174], line 23\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(epochs, min_loss)\u001b[0m\n\u001b[1;32m     20\u001b[0m min_loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minf\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m---> 23\u001b[0m     lossV, lossC, lossB, total_loss \u001b[39m=\u001b[39m training_step()\n\u001b[1;32m     25\u001b[0m     \u001b[39mif\u001b[39;00m total_loss \u001b[39m<\u001b[39m min_loss:\n\u001b[1;32m     26\u001b[0m         lowest_iteration \u001b[39m=\u001b[39m epoch\n",
      "Cell \u001b[0;32mIn[174], line 5\u001b[0m, in \u001b[0;36mtraining_step\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      4\u001b[0m lossV, lossC, lossB, total_loss \u001b[39m=\u001b[39m Loss(batch_size, k_min, k_max)\n\u001b[0;32m----> 5\u001b[0m total_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m      7\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m      9\u001b[0m \u001b[39mreturn\u001b[39;00m lossV, lossC, lossB, total_loss\n",
      "File \u001b[0;32m/if/research-gms/William/jesus/.venv/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/if/research-gms/William/jesus/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1000, 1]] is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "# Run Model (and output loss evolution)\n",
    "lossesV, lossesC, lossesB, total_losses, min_loss_iteration, minimum_loss, best_VF, best_CF = train_model(number_epochs, min_loss)\n",
    "\n",
    "VF.load_state_dict(best_VF.state_dict())\n",
    "CF.load_state_dict(best_CF.state_dict())\n",
    "\n",
    "print('\\n')\n",
    "print(\"Value error: \", lossesV[-1])\n",
    "print(\"Consumption error : \", lossesC[-1])\n",
    "print(\"Boundary error : \", lossesB[-1])\n",
    "print(\"Sum of errors: \", total_losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
